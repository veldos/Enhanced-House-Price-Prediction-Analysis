{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhanced House Price Prediction Analysis\n",
    "\n",
    "This notebook analyzes a housing dataset using multiple advanced models:\n",
    "1. Linear Regression\n",
    "2. Random Forest\n",
    "3. XGBoost\n",
    "4. CatBoost\n",
    "5. LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.ensemble import (RandomForestRegressor, \n",
    "                             StackingRegressor, \n",
    "                             VotingRegressor)\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('Housing.csv')\n",
    "\n",
    "# Display basic information\n",
    "print(\"Dataset Shape:\", df.shape)\n",
    "print(\"\\nFirst few rows:\")\n",
    "display(df.head())\n",
    "\n",
    "# List numerical and categorical columns\n",
    "numerical_columns = ['price', 'area', 'bedrooms', 'bathrooms', 'stories', 'parking']\n",
    "categorical_columns = ['mainroad', 'guestroom', 'basement', 'hotwaterheating', \n",
    "                      'airconditioning', 'prefarea', 'furnishingstatus']\n",
    "\n",
    "print(\"\\nNumerical columns:\", numerical_columns)\n",
    "print(\"Categorical columns:\", categorical_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical features distribution\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, col in enumerate(numerical_columns):\n",
    "    sns.histplot(data=df, x=col, ax=axes[idx])\n",
    "    axes[idx].set_title(f'Distribution of {col}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap for numerical features\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(df[numerical_columns].corr(), annot=True, cmap='coolwarm', center=0)\n",
    "plt.title('Correlation Matrix of Numerical Features')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical features analysis\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 15))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, col in enumerate(categorical_columns):\n",
    "    sns.boxplot(data=df, x=col, y='price', ax=axes[idx])\n",
    "    axes[idx].set_title(f'Price by {col}')\n",
    "    axes[idx].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode categorical variables\n",
    "df_encoded = pd.get_dummies(df, columns=categorical_columns, drop_first=True)\n",
    "\n",
    "# Separate features and target\n",
    "X = df_encoded.drop(['price'], axis=1)\n",
    "y = df_encoded['price']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
    "print(\"Training set shape:\", X_train_scaled.shape)\n",
    "print(\"Testing set shape:\", X_test_scaled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_train, X_test, y_train, y_test, model_name):\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"\\n{model_name} Results:\")\n",
    "    print(f\"RMSE: {rmse:,.2f}\")\n",
    "    print(f\"R2 Score: {r2:.4f}\")\n",
    "    \n",
    "    # Plot actual vs predicted\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "    plt.xlabel('Actual Prices')\n",
    "    plt.ylabel('Predicted Prices')\n",
    "    plt.title(f'{model_name} - Actual vs Predicted Prices')\n",
    "    plt.show()\n",
    "    \n",
    "    return model, rmse, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression\n",
    "lr_model, lr_rmse, lr_r2 = evaluate_model(\n",
    "    LinearRegression(),\n",
    "    X_train_scaled, X_test_scaled,\n",
    "    y_train, y_test,\n",
    "    \"Linear Regression\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "rf_model, rf_rmse, rf_r2 = evaluate_model(\n",
    "    RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    X_train_scaled, X_test_scaled,\n",
    "    y_train, y_test,\n",
    "    \"Random Forest\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importance for Random Forest\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': rf_model.feature_importances_\n",
    "})\n",
    "feature_importance = feature_importance.sort_values('importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=feature_importance.head(15), x='importance', y='feature')\n",
    "plt.title('Top 15 Most Important Features (Random Forest)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Enhanced Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_train, X_test, y_train, y_test, model_name):\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='r2')\n",
    "    \n",
    "    print(f\"\\n{model_name} Results:\")\n",
    "    print(f\"R2 Score: {r2:.4f}\")\n",
    "    # print(f\"RMSE: {rmse:,.2f}\")\n",
    "    # print(f\"MAE: {mae:,.2f}\")\n",
    "    print(f\"Cross-validation R2 Scores: {cv_scores}\")\n",
    "    print(f\"Average CV R2 Score: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "    \n",
    "    # Plot actual vs predicted\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "    plt.xlabel('Actual Prices')\n",
    "    plt.ylabel('Predicted Prices')\n",
    "    plt.title(f'{model_name} - Actual vs Predicted Prices')\n",
    "    plt.show()\n",
    "    \n",
    "    return {'model_name' : model_name,'model': model, 'rmse': rmse, 'mae': mae, 'r2': r2, 'cv_r2': cv_scores.mean()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train all models\n",
    "base_models  = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    'XGBoost': xgb.XGBRegressor(n_estimators=100,learning_rate=0.1, random_state=42),\n",
    "    'LightGBM': lgb.LGBMRegressor(n_estimators=100,learning_rate=0.1,force_col_wise=True, random_state=42),\n",
    "    'CatBoost': CatBoostRegressor(n_estimators=100, random_state=42, verbose=False)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, model in base_models.items():\n",
    "    results[name] = evaluate_model(\n",
    "        model,\n",
    "        X_train_scaled_df , X_test_scaled_df ,\n",
    "        y_train, y_test,\n",
    "        name\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacking = StackingRegressor(\n",
    "    estimators=[\n",
    "        ('rf', base_models['Random Forest']),\n",
    "        ('xgb', base_models['XGBoost']),\n",
    "        ('lgbm', base_models['LightGBM'])\n",
    "    ],\n",
    "    final_estimator=Ridge(alpha=1.0),\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "results['Stacking'] = evaluate_model(\n",
    "    stacking,\n",
    "    X_train_scaled_df, X_test_scaled_df,\n",
    "    y_train, y_test,\n",
    "    \"Stacking (RF+XGB+LGBM)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voting = VotingRegressor([\n",
    "    ('rf', base_models['Random Forest']),\n",
    "    ('xgb', base_models['XGBoost']),\n",
    "    ('lgbm', base_models['LightGBM'])\n",
    "])\n",
    "results['Voting'] = evaluate_model(\n",
    "    voting,\n",
    "    X_train_scaled_df, X_test_scaled_df,\n",
    "    y_train, y_test,\n",
    "    \"Voting (RF+XGB+LGBM)\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "import numpy as np\n",
    "\n",
    "class WeightedAverageEnsemble(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, models, weights):\n",
    "        self.models = models\n",
    "        self.weights = weights\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # Convert to DataFrame if numpy array (for LightGBM compatibility)\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            X = pd.DataFrame(X, columns=[f\"feature_{i}\" for i in range(X.shape[1])])\n",
    "            \n",
    "        for name, model in self.models.items():\n",
    "            model.fit(X, y)\n",
    "        return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        # Convert to DataFrame if numpy array\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            X = pd.DataFrame(X, columns=[f\"feature_{i}\" for i in range(X.shape[1])])\n",
    "            \n",
    "        predictions = np.zeros(X.shape[0])\n",
    "        for (name, model), weight in zip(self.models.items(), self.weights):\n",
    "            predictions += weight * model.predict(X)\n",
    "        return predictions\n",
    "    \n",
    "    def get_params(self, deep=True):\n",
    "        # Required for sklearn compatibility\n",
    "        return {\n",
    "            'models': self.models,\n",
    "            'weights': self.weights\n",
    "        }\n",
    "    \n",
    "    def set_params(self, **params):\n",
    "        # Required for sklearn compatibility\n",
    "        for parameter, value in params.items():\n",
    "            setattr(self, parameter, value)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_ensemble = WeightedAverageEnsemble(\n",
    "    models={\n",
    "        'xgb': xgb.XGBRegressor(n_estimators=100, random_state=42),\n",
    "        'lgbm': lgb.LGBMRegressor(n_estimators=100, random_state=42, verbose=-1),\n",
    "        'rf': RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    },\n",
    "    weights=[0.4, 0.3, 0.3]\n",
    ")\n",
    "\n",
    "# Now works with evaluate_model()\n",
    "results['WeightedAvg'] = evaluate_model(\n",
    "    weighted_ensemble,\n",
    "    X_train_scaled, X_test_scaled,  # Can pass numpy arrays or DataFrames\n",
    "    y_train, y_test,\n",
    "    \"Weighted Average Ensemble\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(degree=2, interaction_only=True)\n",
    "X_train_poly = poly.fit_transform(X_train)\n",
    "X_test_poly = poly.transform(X_test)\n",
    "\n",
    "stacking_poly = StackingRegressor(\n",
    "    estimators=[\n",
    "        ('rf', base_models['Random Forest']),\n",
    "        ('xgb', base_models['XGBoost']),\n",
    "        ('ca', base_models['CatBoost'])\n",
    "    ],\n",
    "    final_estimator=Ridge(alpha=1.0),\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "results['Stacking_Poly'] = evaluate_model(\n",
    "    stacking_poly,\n",
    "    X_train_poly, X_test_poly,\n",
    "    y_train, y_test,\n",
    "    \"Stacking with Polynomial Features\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataframe\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': [result['model_name'] for result in results.values()],\n",
    "    'RMSE': [result['rmse'] for result in results.values()],\n",
    "    'MAE': [result['mae'] for result in results.values()],\n",
    "    'R2 Score': [result['r2'] for result in results.values()],\n",
    "    'CV R2 Score': [result['cv_r2'] for result in results.values()]\n",
    "})\n",
    "\n",
    "print(\"Model Performance Comparison:\")\n",
    "display(comparison_df.sort_values('R2 Score', ascending=False))\n",
    "\n",
    "# Visualize model comparison\n",
    "metrics = ['RMSE', 'MAE', 'R2 Score', 'CV R2 Score']\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, metric in enumerate(metrics):\n",
    "    sns.barplot(data=comparison_df, x='Model', y=metric, ax=axes[idx])\n",
    "    axes[idx].set_title(f'Model Comparison - {metric}')\n",
    "    axes[idx].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Importance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importance(model, model_name):\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        importance = model.feature_importances_\n",
    "    elif hasattr(model, 'coef_'):\n",
    "        importance = np.abs(model.coef_)\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    feature_imp = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'importance': importance\n",
    "    })\n",
    "    feature_imp = feature_imp.sort_values('importance', ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(data=feature_imp.head(10), x='importance', y='feature')\n",
    "    plt.title(f'Top 10 Important Features - {model_name}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot feature importance for each model\n",
    "for name, result in results.items():\n",
    "    plot_feature_importance(result['model'], name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Best Model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best performing model based on R2 score\n",
    "best_model_name = comparison_df.loc[comparison_df['R2 Score'].idxmax(), 'Model']\n",
    "best_model = results[best_model_name]['model']\n",
    "\n",
    "print(f\"Best performing model: {best_model_name}\")\n",
    "\n",
    "# Make predictions with the best model\n",
    "y_pred_best = best_model.predict(X_test_scaled)\n",
    "\n",
    "# Calculate prediction intervals for the best model (if it's not Linear Regression)\n",
    "if isinstance(best_model, LinearRegression):\n",
    "    print(\"\\nPrediction intervals not available for Linear Regression\")\n",
    "else:\n",
    "    # Create a sample prediction with confidence intervals\n",
    "    sample_indices = np.random.choice(len(X_test), 5, replace=False)\n",
    "    sample_X = X_test_scaled[sample_indices]\n",
    "    sample_y = y_test.iloc[sample_indices]\n",
    "    \n",
    "    predictions = []\n",
    "    for _ in range(100):\n",
    "        if hasattr(best_model, 'random_state'):\n",
    "            best_model.random_state = np.random.randint(1000)\n",
    "        predictions.append(best_model.predict(sample_X))\n",
    "    \n",
    "    predictions = np.array(predictions)\n",
    "    mean_pred = predictions.mean(axis=0)\n",
    "    std_pred = predictions.std(axis=0)\n",
    "    \n",
    "    print(\"\\nSample Predictions with Confidence Intervals:\")\n",
    "    for i in range(len(sample_X)):\n",
    "        print(f\"Actual: {sample_y.iloc[i]:,.0f}\")\n",
    "        print(f\"Predicted: {mean_pred[i]:,.0f} +/- {2*std_pred[i]:,.0f}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Projectenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
